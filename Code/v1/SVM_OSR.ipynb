{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92d86d8c",
   "metadata": {},
   "source": [
    "MIT License\n",
    "\n",
    "Copyright (c) 2022 John Shahla\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0321b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as nnf\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# SVM for multi-class classification using one-vs-rest\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# K-Fold Validation\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ccc4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NROWS = 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba38f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and clean X-IIoTID Dataset\n",
    "\n",
    "class DatasetHandler():\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        self.data = pd.read_csv(PATH_CSV,nrows = NROWS)\n",
    "\n",
    "        # Encode categorical columns\n",
    "        self.data = pd.get_dummies(self.data, columns=['Protocol', 'Service'])\n",
    "        \n",
    "        # Defining target class\n",
    "        self.y = self.data['class2']\n",
    "        \n",
    "\n",
    "\n",
    "        self.data = self.data.drop(['Date', 'Timestamp', 'Scr_IP', 'Des_IP', 'class1', 'class2', 'class3'], axis=1)\n",
    "        \n",
    "        # Defining non-numbers values\n",
    "        self.data = self.data.replace({False: 0, 'FALSE': 0, 'false': 0, True: 1, 'TRUE': 1, 'true': 1, '-': np.nan, '?': np.nan, '' : np.nan, ' ' : np.nan}).replace({'[A-Za-z]': np.nan}, regex=True)\n",
    "        \n",
    "        # Delete all columns that has 30%+ of NaN values in all their records\n",
    "        for col in self.data.columns:\n",
    "            count = self.data[col].isna().sum()\n",
    "            \n",
    "            if count/NROWS * 100 > 30.0:\n",
    "                self.data = self.data.drop(col, axis=1)\n",
    "        \n",
    "        # Remove any record that has NaN in it's features and resetting index for data\n",
    "        to_del = []\n",
    "        for col in self.data.columns:\n",
    "            to_del.append(self.data[self.data[col].isin([np.nan])].index.tolist())\n",
    "\n",
    "        to_del =  [j for sub in to_del for j in sub]\n",
    "\n",
    "        self.data = self.data.drop(to_del, axis=0)\n",
    "        self.data = self.data.reset_index()\n",
    "        self.X_data= self.data\n",
    "        self.y = self.y.drop(to_del, axis=0)\n",
    "        self.encoded_labels = self.y.unique()\n",
    "\n",
    "        \n",
    "        # Encode target class with LabelEncoder\n",
    "        self.le = LabelEncoder()\n",
    "        self.y_data = self.le.fit_transform(self.y)\n",
    "        \n",
    "        # Convert all features into float32 for the neural network\n",
    "        self.X_data = self.X_data.astype('float32')     \n",
    "\n",
    "        # Convert training subset-dataset and deleted subset-dataset into ndarray\n",
    "        self.X_data = self.X_data.values\n",
    "\n",
    "        # Scaling features to unit variance\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.X_data = self.scaler.fit_transform(self.X_data)\n",
    "\n",
    "        self.X_stored = self.X_data\n",
    "        self.y_stored = self.y_data\n",
    "\n",
    "        \n",
    "    # Splitting our dataset into training and testing datasets\n",
    "    def get_imbalanced_splits(self, min_num_rec):\n",
    "        \n",
    "        self.imbalancedSamplesX = []\n",
    "        self.imbalancedSamplesY = []    \n",
    "        \n",
    "        self.X_data = self.X_stored\n",
    "        self.y_data= self.y_stored\n",
    "        \n",
    "        # Remove the class_to_del class from class2 from training dataset in order to test it vs OSR model\n",
    "        to_del2 = np.where(self.y_stored == 3)[0]\n",
    "        to_del3 = [i for i in to_del2]\n",
    "\n",
    "        self.x_deleted = self.X_stored[to_del3]        \n",
    "        self.X_data = np.delete(self.X_data,to_del3,axis = 0)\n",
    "\n",
    "        self.y_data = np.delete(self.y_data,to_del3,axis = 0)\n",
    "\n",
    "        self.x_deleted = self.scaler.fit_transform(self.x_deleted)\n",
    "        for i in range(len(np.unique(self.y_stored))):\n",
    "            self.s = np.where(self.y_data==i)[0]\n",
    "\n",
    "            if len(self.s) > min_num_rec : \n",
    "                \n",
    "                self.imbalancedSamplesX.extend(self.X_data[self.s[:]])\n",
    "                self.imbalancedSamplesY.extend(self.y_data[self.s[:]])\n",
    "                \n",
    "            continue\n",
    "        \n",
    "        self.imbalancedSamplesX = np.array(self.imbalancedSamplesX)\n",
    "        self.imbalancedSamplesY = np.array(self.imbalancedSamplesY)\n",
    "        self.encoded_labels = self.encoded_labels[[np.unique(self.imbalancedSamplesY)]]\n",
    "        if len(self.imbalancedSamplesY) == 0 :\n",
    "            return -1,-1,-1,-1,-1\n",
    "        \n",
    "        self.imbalancedSamplesY = self.le.fit_transform(self.imbalancedSamplesY)\n",
    "        \n",
    "        self.ibxtrain , self.ibxtest , self.ibytrain, self.ibytest = train_test_split(self.imbalancedSamplesX,self.imbalancedSamplesY, test_size=0.2, random_state=3)\n",
    "        return  self.ibxtrain , self.ibxtest , self.ibytrain, self.ibytest, self.x_deleted\n",
    "    \n",
    "    def get_balanced_splits(self, min_num_rec, max_num_rec): \n",
    "        \n",
    "        self.balancedSamplesX = []\n",
    "        self.balancedSamplesY = []    \n",
    "        \n",
    "        self.X_data = self.X_stored\n",
    "        self.y_data= self.y_stored\n",
    "        \n",
    "        # Remove the class_to_del class from class2 from training dataset in order to test it vs OSR model\n",
    "        to_del2 = np.where(self.y_stored == 3)[0]\n",
    "        to_del3 = [i for i in to_del2]\n",
    "\n",
    "        self.x_deleted = self.X_stored[to_del3]        \n",
    "        self.X_data = np.delete(self.X_data,to_del3,axis = 0)\n",
    "\n",
    "        self.y_data = np.delete(self.y_data,to_del3,axis = 0)\n",
    "\n",
    "        self.x_deleted = self.scaler.fit_transform(self.x_deleted)\n",
    "        for i in range(len(np.unique(self.y_stored))):\n",
    "            self.s = np.where(self.y_data==i)[0]\n",
    "\n",
    "            if len(self.s) > min_num_rec :\n",
    "\n",
    "                try : \n",
    "                    \n",
    "                    self.balancedSamplesX.extend(self.X_data[self.s[:max_num_rec]])\n",
    "                    self.balancedSamplesY.extend(self.y_data[self.s[:max_num_rec]])\n",
    "                except : \n",
    "                    self.balancedSamplesX.extend(self.X_data[self.s])\n",
    "                    self.balancedSamplesY.extend(self.y_data[self.s])\n",
    "                continue\n",
    "        \n",
    "        self.balancedSamplesX = np.array(self.balancedSamplesX)\n",
    "        self.balancedSamplesY = np.array(self.balancedSamplesY)\n",
    "        self.encoded_labels = self.encoded_labels[[np.unique(self.balancedSamplesY)]]\n",
    "        if len(self.balancedSamplesY) == 0 :\n",
    "            return -1,-1,-1,-1,-1\n",
    "        \n",
    "        self.balancedSamplesY = self.le.fit_transform(self.balancedSamplesY)\n",
    "        \n",
    "        self.bxtrain , self.bxtest , self.bytrain, self.bytest = train_test_split(self.balancedSamplesX,self.balancedSamplesY, test_size=0.2, random_state=3)\n",
    "        return  self.bxtrain , self.bxtest , self.bytrain, self.bytest, self.x_deleted\n",
    "        \n",
    "        \n",
    "    #get number of lables \n",
    "    def get_num_labels(self):\n",
    "        return len(np.unique(self.y_data))\n",
    "\n",
    "    #get number of featuers \n",
    "    def get_num_features(self):\n",
    "            return self.X_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c99ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_predict(row, model):\n",
    "    \n",
    "    t = model.predict_proba([row]).tolist()[0]\n",
    "    y = max(t)\n",
    "    k = t.index(y)\n",
    "    return y, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d7b2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the best theshold for the unknown classes \n",
    "# Error rate is True Unknown predictions + False Known predictions \n",
    "# Modfiy true_negative and false_negative for bigger emphasis on error rate \n",
    "# i.g for bigger emphasis on true_negative accuracy (True unkown predictions) use : true_negative  * 2 \n",
    "\n",
    "def svm_get_threshold (X_data, x_deleted, range_thresh, model):\n",
    "    \n",
    "    lowest_error = 0 \n",
    "    best_thresh = 0 \n",
    "    min_error = 100\n",
    "    \n",
    "    # Change the len for more accurate reults\n",
    "    # The bigger range the more accurate results \n",
    "    deleted_len = 500 if x_deleted.shape[0]>500 else x_deleted.shape[0]\n",
    "    \n",
    "    \n",
    "    # Plotting true_negative (true unknown classes) and false_negative (false known classes) with thresholds \n",
    "    thresh_li = []\n",
    "    true_negative_li = []\n",
    "    false_negative_li = []\n",
    "    error_list = [] \n",
    "    acc_list = []\n",
    "    pred_del = []\n",
    "    pred_test = [] \n",
    "    \n",
    "    for record_idx in range(deleted_len):\n",
    "        pred_del.append(svm_predict(x_deleted[record_idx],model)[0])\n",
    "        pred_test.append(svm_predict(X_data[record_idx],model)[0])\n",
    "    pred_deleted = np.array(pred_del)\n",
    "    pred_tested = np.array(pred_test)\n",
    "\n",
    "    for thresh in [th/1000 for th in range(range_thresh[0],range_thresh[1])]:\n",
    "        error_combined  = 0 \n",
    "        true_negative = 0 \n",
    "        false_negative  = 0 \n",
    "        thresh_li.append(thresh)\n",
    "        true_negative = sum(pred_deleted < thresh)\n",
    "        false_negative = sum(pred_tested < thresh)\n",
    "        true_negative_li.append(100 -true_negative/deleted_len*100)\n",
    "        false_negative_li.append(false_negative/deleted_len*100)\n",
    "        error_combined = (100 - true_negative/deleted_len*100 + false_negative/deleted_len*100 )/2\n",
    "        error_list.append(error_combined)\n",
    "        if error_combined < min_error :\n",
    "            min_error = error_combined\n",
    "            best_thresh = thresh \n",
    "            lowest_error  = min_error\n",
    "            acc_list = [true_negative/deleted_len*100, false_negative/deleted_len*100]\n",
    "\n",
    "    return lowest_error/100, best_thresh, acc_list, thresh_li, true_negative_li, false_negative_li, error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abae225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_draw_thresh_plots(thresh_li , true_negative_li, false_negative_li , error_list):\n",
    "    plt.plot(thresh_li,false_negative_li, label = 'False Negative (False Known Predictions)')\n",
    "    plt.plot(thresh_li,true_negative_li ,label = 'True Negative (True Unknown Predictions)')\n",
    "    plt.plot(thresh_li, error_list , label = 'Error rate (For Both Knowns and Unknowns)' )\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.title('Thresh/Error rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5543f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced Testing\n",
    "\n",
    "df = DatasetHandler()\n",
    "\n",
    "modell = SVC(probability = True)\n",
    "\n",
    "# define ovo strategy\n",
    "ovo = OneVsRestClassifier(modell)\n",
    "\n",
    "X_train , X_test , y_train , y_test, xdel = df.get_balanced_splits(300, 500)\n",
    "\n",
    "# fit model\n",
    "ovo.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_predd = ovo.predict(X_test)\n",
    "\n",
    "lowest_error, best_thresh, acc_list, thresh_li, true_negative_li, false_negative_li, error_list = svm_get_threshold(X_test, xdel, [100,999], ovo)\n",
    "svm_draw_thresh_plots(thresh_li , true_negative_li, false_negative_li, error_list)\n",
    "\n",
    "classification = metrics.classification_report(y_test, y_predd)\n",
    "\n",
    "# Define K-Fold 10\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\n",
    "# Evaluate model\n",
    "scores = cross_val_score(modell, X_test, y_test, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# Report performance\n",
    "res =  classification + '\\n' + 'With k-fold=10 Accuracy: %.3f (%.3f)' % (mean(scores), std(scores))\n",
    "res = res + '\\n \\n' + 'Lowest Error ' + str(lowest_error) \n",
    "res = res + '\\n \\n' + 'Best Thresh ' + str(best_thresh)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05d31d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imbalanced Testing\n",
    "\n",
    "df = DatasetHandler()\n",
    "\n",
    "modell = SVC(probability = True)\n",
    "\n",
    "# define ovo strategy\n",
    "ovo = OneVsRestClassifier(modell)\n",
    "\n",
    "X_train , X_test , y_train , y_test, xdel = df.get_imbalanced_splits(3000)\n",
    "\n",
    "# fit model\n",
    "ovo.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_predd = ovo.predict(X_test)\n",
    "\n",
    "lowest_error, best_thresh, acc_list, thresh_li, true_negative_li, false_negative_li, error_list = svm_get_threshold(X_test, xdel, [100,999], ovo)\n",
    "svm_draw_thresh_plots(thresh_li , true_negative_li, false_negative_li, error_list)\n",
    "\n",
    "classification = metrics.classification_report(y_test, y_predd)\n",
    "\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\n",
    "# Evaluate model\n",
    "scores = cross_val_score(modell, X_test, y_test, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# Report performance\n",
    "res =  classification + '\\n' + 'With k-fold=10 Accuracy: %.3f (%.3f)' % (mean(scores), std(scores))\n",
    "res = res + '\\n \\n' + 'Lowest Error ' + str(lowest_error) \n",
    "res = res + '\\n \\n' + 'Best Thresh ' + str(best_thresh)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27779d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a testing shuffled dataset (contains known and unknown classes) to try and predict it's classes.\n",
    "\n",
    "test_1 = pd.DataFrame(df.X_data[:1000])\n",
    "\n",
    "test_2 = pd.DataFrame(df.x_deleted[:1000])\n",
    "\n",
    "x_test_known_unknows = pd.concat([test_1, test_2], axis=0)\n",
    "\n",
    "x_test_known_unknows = x_test_known_unknows.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "x_test_known_unknows = np.array(x_test_known_unknows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b025ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all(i, row, model, thresh):\n",
    "    if max(model.predict_proba(row).tolist()[0]) < thresh :\n",
    "        print(\"Unkown Class\")\n",
    "    else : \n",
    "        print(df.encoded_labels[df.y_data[i]-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e845398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2000):\n",
    "    predict_all(i, [x_test_known_unknows[i]], ovo, best_thresh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
